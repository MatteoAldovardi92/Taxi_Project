{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyML+cVfW0QQ8Kl0qQkHTf2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoAldovardi92/Taxi_Project/blob/main/NeuralTaxiNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbU4uk9s3zzb",
        "outputId": "236ae521-3dc1-4743-fa18-9c114eb083b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset URL: https://www.kaggle.com/datasets/mnavas/taxi-routes-for-mexico-city-and-quito\n",
            "License(s): CC-BY-SA-4.0\n",
            "Archive:  taxi-routes-for-mexico-city-and-quito.zip\n",
            "  inflating: all-data_clean.csv      \n",
            "  inflating: bog_clean.csv           \n",
            "  inflating: mex_clean.csv           \n",
            "  inflating: uio_clean.csv           \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "!kaggle datasets download -d mnavas/taxi-routes-for-mexico-city-and-quito\n",
        "\n",
        "!unzip taxi-routes-for-mexico-city-and-quito.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('mex_clean.csv')\n"
      ],
      "metadata": {
        "id": "jq_dYb-F60FB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKjv16hW4u_w",
        "outputId": "3285935c-b05a-4292-e0cf-90c970825c7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n",
              "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
              "       'dropoff_latitude', 'store_and_fwd_flag', 'trip_duration',\n",
              "       'dist_meters', 'wait_sec'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',  'store_and_fwd_flag'],\n",
        "        axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "YPPd0meW5yY4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "s9C9ytAR6f0a",
        "outputId": "9250d985-5b72-4c87-a69e-b91634e3af40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
              "0        -99.097369        19.416874         -99.202729         19.430353   \n",
              "1        -99.297148        19.322128         -99.289949         19.326538   \n",
              "2        -99.289603        19.326263         -99.271874         19.328530   \n",
              "3        -99.271161        19.328875         -99.279900         19.326256   \n",
              "4        -99.282761        19.326944         -99.291705         19.322754   \n",
              "\n",
              "   trip_duration  dist_meters  wait_sec  \n",
              "0         120449        12373       242  \n",
              "1          14110         1700       461  \n",
              "2            681         2848       129  \n",
              "3            436         1409       106  \n",
              "4            442         1567        85  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-468871fb-a6d9-46ca-bb19-6d0c13ff20f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>trip_duration</th>\n",
              "      <th>dist_meters</th>\n",
              "      <th>wait_sec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-99.097369</td>\n",
              "      <td>19.416874</td>\n",
              "      <td>-99.202729</td>\n",
              "      <td>19.430353</td>\n",
              "      <td>120449</td>\n",
              "      <td>12373</td>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-99.297148</td>\n",
              "      <td>19.322128</td>\n",
              "      <td>-99.289949</td>\n",
              "      <td>19.326538</td>\n",
              "      <td>14110</td>\n",
              "      <td>1700</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-99.289603</td>\n",
              "      <td>19.326263</td>\n",
              "      <td>-99.271874</td>\n",
              "      <td>19.328530</td>\n",
              "      <td>681</td>\n",
              "      <td>2848</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-99.271161</td>\n",
              "      <td>19.328875</td>\n",
              "      <td>-99.279900</td>\n",
              "      <td>19.326256</td>\n",
              "      <td>436</td>\n",
              "      <td>1409</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-99.282761</td>\n",
              "      <td>19.326944</td>\n",
              "      <td>-99.291705</td>\n",
              "      <td>19.322754</td>\n",
              "      <td>442</td>\n",
              "      <td>1567</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-468871fb-a6d9-46ca-bb19-6d0c13ff20f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-468871fb-a6d9-46ca-bb19-6d0c13ff20f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-468871fb-a6d9-46ca-bb19-6d0c13ff20f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2229c3de-82a1-480f-893e-883ecc4db66f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2229c3de-82a1-480f-893e-883ecc4db66f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2229c3de-82a1-480f-893e-883ecc4db66f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12694,\n  \"fields\": [\n    {\n      \"column\": \"pickup_longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4300007982342574,\n        \"min\": -108.98506900492,\n        \"max\": -86.8713472405748,\n        \"num_unique_values\": 12471,\n        \"samples\": [\n          -99.1522631514294,\n          -99.1777215662366,\n          -99.0939994402444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pickup_latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12951797852274738,\n        \"min\": 18.8291166823126,\n        \"max\": 25.7529644938426,\n        \"num_unique_values\": 12447,\n        \"samples\": [\n          19.3650516718806,\n          19.4338726411099,\n          19.3956240190785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropoff_longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42958163992090564,\n        \"min\": -108.987429265839,\n        \"max\": -86.8710023235055,\n        \"num_unique_values\": 12588,\n        \"samples\": [\n          -99.2351201,\n          -99.1333854820755,\n          -99.1902737092966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropoff_latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13119871003659653,\n        \"min\": 18.82910342,\n        \"max\": 25.7810576230775,\n        \"num_unique_values\": 12585,\n        \"samples\": [\n          19.47110564,\n          19.4887428896653,\n          19.481683404768\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trip_duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 229336,\n        \"min\": 2,\n        \"max\": 16570949,\n        \"num_unique_values\": 4950,\n        \"samples\": [\n          651,\n          4140,\n          879\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dist_meters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11386,\n        \"min\": 21,\n        \"max\": 802537,\n        \"num_unique_values\": 8041,\n        \"samples\": [\n          1631,\n          4396,\n          8375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wait_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57996115,\n        \"min\": 0,\n        \"max\": 4429346278,\n        \"num_unique_values\": 2441,\n        \"samples\": [\n          1638,\n          496,\n          1932\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparation of the dataset:\n",
        "X = df.drop('trip_duration', axis=1)\n",
        "y = df['trip_duration']"
      ],
      "metadata": {
        "id": "6v_Mw--x66Ia"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Hdc7ymHB7cNf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "n_hidden = 10\n"
      ],
      "metadata": {
        "id": "iLXqNZjg94Gf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(6, n_hidden),\n",
        "    nn.BatchNorm1d(n_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(n_hidden, 1)  # For regression or binary classification\n",
        ").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcAtnxTTHU2K",
        "outputId": "adc566a9-1989-485c-b646-243ac6e4950c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move data to GPU\n",
        "X_train_tensor = torch.tensor(X_train)\n",
        "y_train_tensor = torch.tensor(y_train.values)\n",
        "X_test_tensor = torch.tensor(X_test)\n",
        "y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Move data to GPU and ensure consistent dtype\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32, device=device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "IXzRObE3Hb_a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â Training loop"
      ],
      "metadata": {
        "id": "sp5E-3RPOupJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model\n",
        "n_features = X_train_tensor.shape[1] # This way I can easily modify the number of features\n",
        "n_hidden = 10\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features, n_hidden),\n",
        "    nn.BatchNorm1d(n_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(n_hidden, 1)\n",
        ").to(device)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32, device=device),\n",
        "                                       torch.tensor(y_train.values, dtype=torch.float32, device=device)),\n",
        "                          batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Track losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "n_epochs = 2000000\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(batch_X)\n",
        "        loss = criterion(preds, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_test_tensor)\n",
        "        val_loss = criterion(val_preds, y_test_tensor).item()\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ADH6Q2YdNrGG",
        "outputId": "c538313a-fcb9-4e91-f37c-c82870d7951b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000000 | Train Loss: 31929232375.7539 | Val Loss: 137622634496.0000\n",
            "Epoch 2/2000000 | Train Loss: 32011880039.9756 | Val Loss: 137622536192.0000\n",
            "Epoch 3/2000000 | Train Loss: 31929709500.2020 | Val Loss: 137622372352.0000\n",
            "Epoch 4/2000000 | Train Loss: 31924954445.5908 | Val Loss: 137622126592.0000\n",
            "Epoch 5/2000000 | Train Loss: 31925481891.0134 | Val Loss: 137621798912.0000\n",
            "Epoch 6/2000000 | Train Loss: 31924371216.0653 | Val Loss: 137621340160.0000\n",
            "Epoch 7/2000000 | Train Loss: 31924519929.3125 | Val Loss: 137620865024.0000\n",
            "Epoch 8/2000000 | Train Loss: 31923518501.2036 | Val Loss: 137620307968.0000\n",
            "Epoch 9/2000000 | Train Loss: 32025126623.2917 | Val Loss: 137619570688.0000\n",
            "Epoch 10/2000000 | Train Loss: 31922456664.8151 | Val Loss: 137618964480.0000\n",
            "Epoch 11/2000000 | Train Loss: 31923213909.2830 | Val Loss: 137618210816.0000\n",
            "Epoch 12/2000000 | Train Loss: 31922988545.7429 | Val Loss: 137617326080.0000\n",
            "Epoch 13/2000000 | Train Loss: 32467861184.9186 | Val Loss: 137616654336.0000\n",
            "Epoch 14/2000000 | Train Loss: 31919881569.2940 | Val Loss: 137615753216.0000\n",
            "Epoch 15/2000000 | Train Loss: 31919182788.8121 | Val Loss: 137614999552.0000\n",
            "Epoch 16/2000000 | Train Loss: 31918422427.6364 | Val Loss: 137613950976.0000\n",
            "Epoch 17/2000000 | Train Loss: 31919340874.1046 | Val Loss: 137613213696.0000\n",
            "Epoch 18/2000000 | Train Loss: 31921550901.3101 | Val Loss: 137611870208.0000\n",
            "Epoch 19/2000000 | Train Loss: 31915910177.2276 | Val Loss: 137610821632.0000\n",
            "Epoch 20/2000000 | Train Loss: 31914998438.5051 | Val Loss: 137609560064.0000\n",
            "Epoch 21/2000000 | Train Loss: 31914388634.1619 | Val Loss: 137608904704.0000\n",
            "Epoch 22/2000000 | Train Loss: 31913631476.5495 | Val Loss: 137606791168.0000\n",
            "Epoch 23/2000000 | Train Loss: 32033818861.0676 | Val Loss: 137606184960.0000\n",
            "Epoch 24/2000000 | Train Loss: 31911067628.2362 | Val Loss: 137604104192.0000\n",
            "Epoch 25/2000000 | Train Loss: 31912414335.7862 | Val Loss: 137602940928.0000\n",
            "Epoch 26/2000000 | Train Loss: 31908890762.1584 | Val Loss: 137602285568.0000\n",
            "Epoch 27/2000000 | Train Loss: 31908229285.7923 | Val Loss: 137600581632.0000\n",
            "Epoch 28/2000000 | Train Loss: 31906609523.8852 | Val Loss: 137599139840.0000\n",
            "Epoch 29/2000000 | Train Loss: 31905481444.8294 | Val Loss: 137597943808.0000\n",
            "Epoch 30/2000000 | Train Loss: 31904167605.1376 | Val Loss: 137596026880.0000\n",
            "Epoch 31/2000000 | Train Loss: 31979519390.5668 | Val Loss: 137594257408.0000\n",
            "Epoch 32/2000000 | Train Loss: 31901631889.6030 | Val Loss: 137593651200.0000\n",
            "Epoch 33/2000000 | Train Loss: 31903967947.5047 | Val Loss: 137591914496.0000\n",
            "Epoch 34/2000000 | Train Loss: 31899123842.2343 | Val Loss: 137588539392.0000\n",
            "Epoch 35/2000000 | Train Loss: 31897679794.9481 | Val Loss: 137588129792.0000\n",
            "Epoch 36/2000000 | Train Loss: 32409367140.7803 | Val Loss: 137585262592.0000\n",
            "Epoch 37/2000000 | Train Loss: 31895133806.4037 | Val Loss: 137583804416.0000\n",
            "Epoch 38/2000000 | Train Loss: 31894713970.8512 | Val Loss: 137583214592.0000\n",
            "Epoch 39/2000000 | Train Loss: 31892130577.7174 | Val Loss: 137579642880.0000\n",
            "Epoch 40/2000000 | Train Loss: 31890555945.3340 | Val Loss: 137578790912.0000\n",
            "Epoch 41/2000000 | Train Loss: 31888949291.0189 | Val Loss: 137577168896.0000\n",
            "Epoch 42/2000000 | Train Loss: 31895910262.3847 | Val Loss: 137574678528.0000\n",
            "Epoch 43/2000000 | Train Loss: 31886563209.2280 | Val Loss: 137575022592.0000\n",
            "Epoch 44/2000000 | Train Loss: 31884862067.4275 | Val Loss: 137571893248.0000\n",
            "Epoch 45/2000000 | Train Loss: 31926039664.6334 | Val Loss: 137570582528.0000\n",
            "Epoch 46/2000000 | Train Loss: 31881246341.4513 | Val Loss: 137568452608.0000\n",
            "Epoch 47/2000000 | Train Loss: 31879408416.3011 | Val Loss: 137562095616.0000\n",
            "Epoch 48/2000000 | Train Loss: 31896434765.7186 | Val Loss: 137560948736.0000\n",
            "Epoch 49/2000000 | Train Loss: 31876887337.2005 | Val Loss: 137560113152.0000\n",
            "Epoch 50/2000000 | Train Loss: 31874488890.0220 | Val Loss: 137560948736.0000\n",
            "Epoch 51/2000000 | Train Loss: 35009339317.9756 | Val Loss: 137554771968.0000\n",
            "Epoch 52/2000000 | Train Loss: 31871375326.6333 | Val Loss: 137553821696.0000\n",
            "Epoch 53/2000000 | Train Loss: 31869784520.0472 | Val Loss: 137552363520.0000\n",
            "Epoch 54/2000000 | Train Loss: 31885800575.9914 | Val Loss: 137550036992.0000\n",
            "Epoch 55/2000000 | Train Loss: 31866597858.9744 | Val Loss: 137547268096.0000\n",
            "Epoch 56/2000000 | Train Loss: 31863604716.5263 | Val Loss: 137546629120.0000\n",
            "Epoch 57/2000000 | Train Loss: 31863653497.7917 | Val Loss: 137541632000.0000\n",
            "Epoch 58/2000000 | Train Loss: 31860913255.6112 | Val Loss: 137540780032.0000\n",
            "Epoch 59/2000000 | Train Loss: 37412588447.6961 | Val Loss: 137536897024.0000\n",
            "Epoch 60/2000000 | Train Loss: 31856399547.9815 | Val Loss: 137537093632.0000\n",
            "Epoch 61/2000000 | Train Loss: 31874595664.9678 | Val Loss: 137535078400.0000\n",
            "Epoch 62/2000000 | Train Loss: 31861147281.6667 | Val Loss: 137532473344.0000\n",
            "Epoch 63/2000000 | Train Loss: 31951677870.6745 | Val Loss: 137529475072.0000\n",
            "Epoch 64/2000000 | Train Loss: 31848708604.0739 | Val Loss: 137526525952.0000\n",
            "Epoch 65/2000000 | Train Loss: 31887472286.9627 | Val Loss: 137523462144.0000\n",
            "Epoch 66/2000000 | Train Loss: 31845840341.9744 | Val Loss: 137522708480.0000\n",
            "Epoch 67/2000000 | Train Loss: 31864126798.0165 | Val Loss: 137518858240.0000\n",
            "Epoch 68/2000000 | Train Loss: 31840865452.7905 | Val Loss: 137517187072.0000\n",
            "Epoch 69/2000000 | Train Loss: 31838822834.9222 | Val Loss: 137512452096.0000\n",
            "Epoch 70/2000000 | Train Loss: 31837070761.0102 | Val Loss: 137515515904.0000\n",
            "Epoch 71/2000000 | Train Loss: 31839231743.6415 | Val Loss: 137507635200.0000\n",
            "Epoch 72/2000000 | Train Loss: 31833439519.3836 | Val Loss: 137508667392.0000\n",
            "Epoch 73/2000000 | Train Loss: 31830588236.5110 | Val Loss: 137502261248.0000\n",
            "Epoch 74/2000000 | Train Loss: 31828517864.6352 | Val Loss: 137506291712.0000\n",
            "Epoch 75/2000000 | Train Loss: 31965288126.1808 | Val Loss: 137497083904.0000\n",
            "Epoch 76/2000000 | Train Loss: 31824758645.6447 | Val Loss: 137494609920.0000\n",
            "Epoch 77/2000000 | Train Loss: 31822061518.6392 | Val Loss: 137495724032.0000\n",
            "Epoch 78/2000000 | Train Loss: 31823033845.3648 | Val Loss: 137487532032.0000\n",
            "Epoch 79/2000000 | Train Loss: 31904044379.8538 | Val Loss: 137487990784.0000\n",
            "Epoch 80/2000000 | Train Loss: 31820444680.3899 | Val Loss: 137484500992.0000\n",
            "Epoch 81/2000000 | Train Loss: 31814864637.1981 | Val Loss: 137479962624.0000\n",
            "Epoch 82/2000000 | Train Loss: 31811249552.9717 | Val Loss: 137477914624.0000\n",
            "Epoch 83/2000000 | Train Loss: 31808795055.2626 | Val Loss: 137477423104.0000\n",
            "Epoch 84/2000000 | Train Loss: 31806500353.9623 | Val Loss: 137469771776.0000\n",
            "Epoch 85/2000000 | Train Loss: 31804313434.2720 | Val Loss: 137475178496.0000\n",
            "Epoch 86/2000000 | Train Loss: 31802010723.8789 | Val Loss: 137465626624.0000\n",
            "Epoch 87/2000000 | Train Loss: 31799919272.1336 | Val Loss: 137464823808.0000\n",
            "Epoch 88/2000000 | Train Loss: 31830267991.6714 | Val Loss: 137468608512.0000\n",
            "Epoch 89/2000000 | Train Loss: 31796976217.7406 | Val Loss: 137461137408.0000\n",
            "Epoch 90/2000000 | Train Loss: 31805586847.2421 | Val Loss: 137454026752.0000\n",
            "Epoch 91/2000000 | Train Loss: 31794962887.4009 | Val Loss: 137456467968.0000\n",
            "Epoch 92/2000000 | Train Loss: 31796068880.8176 | Val Loss: 137460219904.0000\n",
            "Epoch 93/2000000 | Train Loss: 31787900155.7531 | Val Loss: 137445425152.0000\n",
            "Epoch 94/2000000 | Train Loss: 31791925941.2547 | Val Loss: 137441460224.0000\n",
            "Epoch 95/2000000 | Train Loss: 31781460421.3962 | Val Loss: 137443540992.0000\n",
            "Epoch 96/2000000 | Train Loss: 31792223004.8050 | Val Loss: 137440002048.0000\n",
            "Epoch 97/2000000 | Train Loss: 31777883184.5645 | Val Loss: 137431457792.0000\n",
            "Epoch 98/2000000 | Train Loss: 31774790902.5252 | Val Loss: 137431711744.0000\n",
            "Epoch 99/2000000 | Train Loss: 31780100863.2972 | Val Loss: 137423077376.0000\n",
            "Epoch 100/2000000 | Train Loss: 31770147044.3019 | Val Loss: 137426206720.0000\n",
            "Epoch 101/2000000 | Train Loss: 31767973294.2296 | Val Loss: 137418317824.0000\n",
            "Epoch 102/2000000 | Train Loss: 32269529558.4214 | Val Loss: 137423224832.0000\n",
            "Epoch 103/2000000 | Train Loss: 31763250000.3711 | Val Loss: 137425698816.0000\n",
            "Epoch 104/2000000 | Train Loss: 31760361952.9025 | Val Loss: 137419980800.0000\n",
            "Epoch 105/2000000 | Train Loss: 31758645739.0252 | Val Loss: 137409626112.0000\n",
            "Epoch 106/2000000 | Train Loss: 31756088114.8585 | Val Loss: 137411084288.0000\n",
            "Epoch 107/2000000 | Train Loss: 31837958697.8239 | Val Loss: 137418424320.0000\n",
            "Epoch 108/2000000 | Train Loss: 31752253163.9277 | Val Loss: 137402351616.0000\n",
            "Epoch 109/2000000 | Train Loss: 31749028252.9151 | Val Loss: 137397993472.0000\n",
            "Epoch 110/2000000 | Train Loss: 31746965491.4182 | Val Loss: 137391439872.0000\n",
            "Epoch 111/2000000 | Train Loss: 31744360390.1635 | Val Loss: 137390047232.0000\n",
            "Epoch 112/2000000 | Train Loss: 31742373789.2484 | Val Loss: 137391022080.0000\n",
            "Epoch 113/2000000 | Train Loss: 31740013030.5912 | Val Loss: 137386573824.0000\n",
            "Epoch 114/2000000 | Train Loss: 31824990966.4465 | Val Loss: 137376858112.0000\n",
            "Epoch 115/2000000 | Train Loss: 31734724318.9403 | Val Loss: 137380610048.0000\n",
            "Epoch 116/2000000 | Train Loss: 31732482309.9560 | Val Loss: 137386385408.0000\n",
            "Epoch 117/2000000 | Train Loss: 31731030321.3711 | Val Loss: 137378684928.0000\n",
            "Epoch 118/2000000 | Train Loss: 31728634983.6981 | Val Loss: 137372270592.0000\n",
            "Epoch 119/2000000 | Train Loss: 31725921140.7170 | Val Loss: 137371844608.0000\n",
            "Epoch 120/2000000 | Train Loss: 31723418247.0629 | Val Loss: 137378578432.0000\n",
            "Epoch 121/2000000 | Train Loss: 31722636623.6667 | Val Loss: 137356771328.0000\n",
            "Epoch 122/2000000 | Train Loss: 31718817256.6667 | Val Loss: 137355558912.0000\n",
            "Epoch 123/2000000 | Train Loss: 31717036144.7610 | Val Loss: 137394036736.0000\n",
            "Epoch 124/2000000 | Train Loss: 31714054481.4025 | Val Loss: 137352593408.0000\n",
            "Epoch 125/2000000 | Train Loss: 31745176821.0000 | Val Loss: 137347014656.0000\n",
            "Epoch 126/2000000 | Train Loss: 31709479645.3333 | Val Loss: 137344884736.0000\n",
            "Epoch 127/2000000 | Train Loss: 31707373740.3459 | Val Loss: 137343934464.0000\n",
            "Epoch 128/2000000 | Train Loss: 31705811116.5535 | Val Loss: 137342230528.0000\n",
            "Epoch 129/2000000 | Train Loss: 31704099842.9057 | Val Loss: 137336463360.0000\n",
            "Epoch 130/2000000 | Train Loss: 31709189438.0566 | Val Loss: 137336709120.0000\n",
            "Epoch 131/2000000 | Train Loss: 31699727309.6981 | Val Loss: 137327468544.0000\n",
            "Epoch 132/2000000 | Train Loss: 31696249022.8679 | Val Loss: 137331818496.0000\n",
            "Epoch 133/2000000 | Train Loss: 31694896345.6541 | Val Loss: 137330933760.0000\n",
            "Epoch 134/2000000 | Train Loss: 31692634557.7484 | Val Loss: 137348562944.0000\n",
            "Epoch 135/2000000 | Train Loss: 31691289043.2390 | Val Loss: 137325010944.0000\n",
            "Epoch 136/2000000 | Train Loss: 31688174097.7862 | Val Loss: 137317023744.0000\n",
            "Epoch 137/2000000 | Train Loss: 31685800097.1761 | Val Loss: 137309954048.0000\n",
            "Epoch 138/2000000 | Train Loss: 31682911212.2830 | Val Loss: 137340436480.0000\n",
            "Epoch 139/2000000 | Train Loss: 31681328623.0252 | Val Loss: 137333211136.0000\n",
            "Epoch 140/2000000 | Train Loss: 31683863600.2516 | Val Loss: 137309536256.0000\n",
            "Epoch 141/2000000 | Train Loss: 32958324900.3082 | Val Loss: 137303367680.0000\n",
            "Epoch 142/2000000 | Train Loss: 31674477890.3648 | Val Loss: 137300590592.0000\n",
            "Epoch 143/2000000 | Train Loss: 31672354802.3208 | Val Loss: 137297149952.0000\n",
            "Epoch 144/2000000 | Train Loss: 31670286187.5597 | Val Loss: 137295904768.0000\n",
            "Epoch 145/2000000 | Train Loss: 31673812557.9371 | Val Loss: 137290039296.0000\n",
            "Epoch 146/2000000 | Train Loss: 31666009581.1321 | Val Loss: 137291743232.0000\n",
            "Epoch 147/2000000 | Train Loss: 31664242722.4277 | Val Loss: 137291874304.0000\n",
            "Epoch 148/2000000 | Train Loss: 31662180185.3962 | Val Loss: 137330589696.0000\n",
            "Epoch 149/2000000 | Train Loss: 31661530332.6289 | Val Loss: 137285648384.0000\n",
            "Epoch 150/2000000 | Train Loss: 31658137054.6792 | Val Loss: 137281585152.0000\n",
            "Epoch 151/2000000 | Train Loss: 31655522953.7736 | Val Loss: 137279447040.0000\n",
            "Epoch 152/2000000 | Train Loss: 31766808689.7610 | Val Loss: 137267404800.0000\n",
            "Epoch 153/2000000 | Train Loss: 31651837940.4654 | Val Loss: 137352101888.0000\n",
            "Epoch 154/2000000 | Train Loss: 31655411881.6855 | Val Loss: 137263521792.0000\n",
            "Epoch 155/2000000 | Train Loss: 31654473338.1509 | Val Loss: 137260597248.0000\n",
            "Epoch 156/2000000 | Train Loss: 31645879025.4088 | Val Loss: 137255141376.0000\n",
            "Epoch 157/2000000 | Train Loss: 31646423206.7547 | Val Loss: 137296478208.0000\n",
            "Epoch 158/2000000 | Train Loss: 31643455643.1572 | Val Loss: 137251127296.0000\n",
            "Epoch 159/2000000 | Train Loss: 31639942672.2013 | Val Loss: 137262694400.0000\n",
            "Epoch 160/2000000 | Train Loss: 31637917035.0440 | Val Loss: 137253773312.0000\n",
            "Epoch 161/2000000 | Train Loss: 31635944346.2264 | Val Loss: 137243189248.0000\n",
            "Epoch 162/2000000 | Train Loss: 31633526132.9308 | Val Loss: 137287204864.0000\n",
            "Epoch 163/2000000 | Train Loss: 31631912516.1509 | Val Loss: 137246203904.0000\n",
            "Epoch 164/2000000 | Train Loss: 31629633546.5157 | Val Loss: 137241321472.0000\n",
            "Epoch 165/2000000 | Train Loss: 31627944545.4717 | Val Loss: 137241321472.0000\n",
            "Epoch 166/2000000 | Train Loss: 31626910442.3899 | Val Loss: 137234628608.0000\n",
            "Epoch 167/2000000 | Train Loss: 31624550158.3270 | Val Loss: 137231278080.0000\n",
            "Epoch 168/2000000 | Train Loss: 31622835510.2390 | Val Loss: 137223413760.0000\n",
            "Epoch 169/2000000 | Train Loss: 31670314800.6415 | Val Loss: 137219555328.0000\n",
            "Epoch 170/2000000 | Train Loss: 31618417059.7862 | Val Loss: 137229467648.0000\n",
            "Epoch 171/2000000 | Train Loss: 31623900030.1006 | Val Loss: 137224052736.0000\n",
            "Epoch 172/2000000 | Train Loss: 32019456369.8239 | Val Loss: 137222479872.0000\n",
            "Epoch 173/2000000 | Train Loss: 31614389131.0189 | Val Loss: 137206784000.0000\n",
            "Epoch 174/2000000 | Train Loss: 31611526223.4340 | Val Loss: 137215868928.0000\n",
            "Epoch 175/2000000 | Train Loss: 31921274639.7736 | Val Loss: 137220112384.0000\n",
            "Epoch 176/2000000 | Train Loss: 31607914488.0377 | Val Loss: 137203310592.0000\n",
            "Epoch 177/2000000 | Train Loss: 31606002102.2138 | Val Loss: 137204203520.0000\n",
            "Epoch 178/2000000 | Train Loss: 31605367018.2390 | Val Loss: 137211346944.0000\n",
            "Epoch 179/2000000 | Train Loss: 31603278269.8113 | Val Loss: 137201819648.0000\n",
            "Epoch 180/2000000 | Train Loss: 31602528682.5409 | Val Loss: 137200754688.0000\n",
            "Epoch 181/2000000 | Train Loss: 31600228693.4843 | Val Loss: 137203851264.0000\n",
            "Epoch 182/2000000 | Train Loss: 31599048465.7107 | Val Loss: 137245442048.0000\n",
            "Epoch 183/2000000 | Train Loss: 31597556398.8176 | Val Loss: 137198616576.0000\n",
            "Epoch 184/2000000 | Train Loss: 31593994400.3522 | Val Loss: 137210429440.0000\n",
            "Epoch 185/2000000 | Train Loss: 31616124643.7233 | Val Loss: 137177612288.0000\n",
            "Epoch 186/2000000 | Train Loss: 31591306164.1761 | Val Loss: 137188294656.0000\n",
            "Epoch 187/2000000 | Train Loss: 31589670583.1950 | Val Loss: 137249071104.0000\n",
            "Epoch 188/2000000 | Train Loss: 31594537097.9623 | Val Loss: 137221685248.0000\n",
            "Epoch 189/2000000 | Train Loss: 31587902493.0314 | Val Loss: 137184337920.0000\n",
            "Epoch 190/2000000 | Train Loss: 31584904013.1069 | Val Loss: 137169444864.0000\n",
            "Epoch 191/2000000 | Train Loss: 31583619397.7107 | Val Loss: 137174425600.0000\n",
            "Epoch 192/2000000 | Train Loss: 31581661153.9874 | Val Loss: 137163104256.0000\n",
            "Epoch 193/2000000 | Train Loss: 31580374498.7421 | Val Loss: 137169494016.0000\n",
            "Epoch 194/2000000 | Train Loss: 31590009240.1006 | Val Loss: 137172205568.0000\n",
            "Epoch 195/2000000 | Train Loss: 31578009095.8239 | Val Loss: 137156329472.0000\n",
            "Epoch 196/2000000 | Train Loss: 31576808369.4088 | Val Loss: 137205358592.0000\n",
            "Epoch 197/2000000 | Train Loss: 31574880573.1321 | Val Loss: 137166684160.0000\n",
            "Epoch 198/2000000 | Train Loss: 31583128762.1384 | Val Loss: 137141116928.0000\n",
            "Epoch 199/2000000 | Train Loss: 31575169592.8302 | Val Loss: 137159081984.0000\n",
            "Epoch 200/2000000 | Train Loss: 31570024870.2390 | Val Loss: 137156722688.0000\n",
            "Epoch 201/2000000 | Train Loss: 31570256921.5597 | Val Loss: 137144721408.0000\n",
            "Epoch 202/2000000 | Train Loss: 31567900719.2201 | Val Loss: 137148628992.0000\n",
            "Epoch 203/2000000 | Train Loss: 31566068119.8742 | Val Loss: 137166807040.0000\n",
            "Epoch 204/2000000 | Train Loss: 31565376448.4277 | Val Loss: 137133916160.0000\n",
            "Epoch 205/2000000 | Train Loss: 31596663757.9623 | Val Loss: 137141911552.0000\n",
            "Epoch 206/2000000 | Train Loss: 31561796177.5346 | Val Loss: 137135169536.0000\n",
            "Epoch 207/2000000 | Train Loss: 31560064666.4403 | Val Loss: 137147113472.0000\n",
            "Epoch 208/2000000 | Train Loss: 31560146142.8679 | Val Loss: 137181306880.0000\n",
            "Epoch 209/2000000 | Train Loss: 31558307089.9874 | Val Loss: 137151397888.0000\n",
            "Epoch 210/2000000 | Train Loss: 31556916087.9748 | Val Loss: 137128247296.0000\n",
            "Epoch 211/2000000 | Train Loss: 31555494314.7673 | Val Loss: 137159540736.0000\n",
            "Epoch 212/2000000 | Train Loss: 31566082164.4025 | Val Loss: 137118949376.0000\n",
            "Epoch 213/2000000 | Train Loss: 31554449292.0000 | Val Loss: 137114943488.0000\n",
            "Epoch 214/2000000 | Train Loss: 31551742711.6478 | Val Loss: 137128615936.0000\n",
            "Epoch 215/2000000 | Train Loss: 31552522813.7358 | Val Loss: 137126600704.0000\n",
            "Epoch 216/2000000 | Train Loss: 31549462643.4214 | Val Loss: 137124790272.0000\n",
            "Epoch 217/2000000 | Train Loss: 31548488068.1509 | Val Loss: 137309405184.0000\n",
            "Epoch 218/2000000 | Train Loss: 33452585766.8931 | Val Loss: 137105006592.0000\n",
            "Epoch 219/2000000 | Train Loss: 31545771434.6164 | Val Loss: 137202860032.0000\n",
            "Epoch 220/2000000 | Train Loss: 31544666991.7987 | Val Loss: 137110355968.0000\n",
            "Epoch 221/2000000 | Train Loss: 31543291452.2013 | Val Loss: 137095946240.0000\n",
            "Epoch 222/2000000 | Train Loss: 31541962717.3082 | Val Loss: 137114492928.0000\n",
            "Epoch 223/2000000 | Train Loss: 31544388255.0189 | Val Loss: 137077932032.0000\n",
            "Epoch 224/2000000 | Train Loss: 31538980638.3396 | Val Loss: 137107922944.0000\n",
            "Epoch 225/2000000 | Train Loss: 31542399960.1258 | Val Loss: 137115721728.0000\n",
            "Epoch 226/2000000 | Train Loss: 31537513662.0881 | Val Loss: 137137496064.0000\n",
            "Epoch 227/2000000 | Train Loss: 31537253530.3145 | Val Loss: 137108054016.0000\n",
            "Epoch 228/2000000 | Train Loss: 31535600110.7170 | Val Loss: 137096224768.0000\n",
            "Epoch 229/2000000 | Train Loss: 31534628283.7736 | Val Loss: 137092620288.0000\n",
            "Epoch 230/2000000 | Train Loss: 31532803720.8050 | Val Loss: 137089859584.0000\n",
            "Epoch 231/2000000 | Train Loss: 31532072036.1509 | Val Loss: 137150652416.0000\n",
            "Epoch 232/2000000 | Train Loss: 31531668259.3208 | Val Loss: 137097756672.0000\n",
            "Epoch 233/2000000 | Train Loss: 31531454048.2516 | Val Loss: 137091997696.0000\n",
            "Epoch 234/2000000 | Train Loss: 31529651625.6604 | Val Loss: 137089245184.0000\n",
            "Epoch 235/2000000 | Train Loss: 31529922526.8931 | Val Loss: 137073475584.0000\n",
            "Epoch 236/2000000 | Train Loss: 31526968303.4465 | Val Loss: 137188524032.0000\n",
            "Epoch 237/2000000 | Train Loss: 31526883554.4654 | Val Loss: 137081274368.0000\n",
            "Epoch 238/2000000 | Train Loss: 31526228017.2075 | Val Loss: 137100402688.0000\n",
            "Epoch 239/2000000 | Train Loss: 31525023162.3396 | Val Loss: 137093382144.0000\n",
            "Epoch 240/2000000 | Train Loss: 31590852306.7170 | Val Loss: 137075015680.0000\n",
            "Epoch 241/2000000 | Train Loss: 31523466522.5660 | Val Loss: 137072918528.0000\n",
            "Epoch 242/2000000 | Train Loss: 31522017331.5220 | Val Loss: 137057845248.0000\n",
            "Epoch 243/2000000 | Train Loss: 31521145810.6164 | Val Loss: 137157124096.0000\n",
            "Epoch 244/2000000 | Train Loss: 31521064026.1132 | Val Loss: 137061212160.0000\n",
            "Epoch 245/2000000 | Train Loss: 31530356377.0566 | Val Loss: 137081954304.0000\n",
            "Epoch 246/2000000 | Train Loss: 31541008265.3082 | Val Loss: 137052954624.0000\n",
            "Epoch 247/2000000 | Train Loss: 31517780839.5975 | Val Loss: 137063792640.0000\n",
            "Epoch 248/2000000 | Train Loss: 31516776540.3774 | Val Loss: 137377218560.0000\n",
            "Epoch 249/2000000 | Train Loss: 31517292944.9057 | Val Loss: 137131458560.0000\n",
            "Epoch 250/2000000 | Train Loss: 31515274209.2579 | Val Loss: 137064751104.0000\n",
            "Epoch 251/2000000 | Train Loss: 31515179584.6038 | Val Loss: 137067249664.0000\n",
            "Epoch 252/2000000 | Train Loss: 31513155784.1509 | Val Loss: 137084215296.0000\n",
            "Epoch 253/2000000 | Train Loss: 31518433808.0000 | Val Loss: 137060745216.0000\n",
            "Epoch 254/2000000 | Train Loss: 31512756177.2579 | Val Loss: 137044647936.0000\n",
            "Epoch 255/2000000 | Train Loss: 31510871310.3899 | Val Loss: 137089294336.0000\n",
            "Epoch 256/2000000 | Train Loss: 31511836008.0000 | Val Loss: 137059909632.0000\n",
            "Epoch 257/2000000 | Train Loss: 31511786896.3522 | Val Loss: 137106284544.0000\n",
            "Epoch 258/2000000 | Train Loss: 31508803321.0566 | Val Loss: 137151938560.0000\n",
            "Epoch 259/2000000 | Train Loss: 31509068471.4465 | Val Loss: 137037291520.0000\n",
            "Epoch 260/2000000 | Train Loss: 31507893129.8616 | Val Loss: 137276522496.0000\n",
            "Epoch 261/2000000 | Train Loss: 31507564835.9748 | Val Loss: 137234718720.0000\n",
            "Epoch 262/2000000 | Train Loss: 31505328480.2516 | Val Loss: 137075843072.0000\n",
            "Epoch 263/2000000 | Train Loss: 31506072521.5094 | Val Loss: 137053093888.0000\n",
            "Epoch 264/2000000 | Train Loss: 31540035528.0503 | Val Loss: 137084854272.0000\n",
            "Epoch 265/2000000 | Train Loss: 31505079119.0440 | Val Loss: 137048915968.0000\n",
            "Epoch 266/2000000 | Train Loss: 31505123733.6855 | Val Loss: 137080741888.0000\n",
            "Epoch 267/2000000 | Train Loss: 31503387783.6981 | Val Loss: 137071534080.0000\n",
            "Epoch 268/2000000 | Train Loss: 31502820062.7421 | Val Loss: 137053749248.0000\n",
            "Epoch 269/2000000 | Train Loss: 31502347226.5157 | Val Loss: 137034006528.0000\n",
            "Epoch 270/2000000 | Train Loss: 31501849563.8742 | Val Loss: 137025249280.0000\n",
            "Epoch 271/2000000 | Train Loss: 31558027612.1258 | Val Loss: 137181904896.0000\n",
            "Epoch 272/2000000 | Train Loss: 31499270154.3648 | Val Loss: 137033670656.0000\n",
            "Epoch 273/2000000 | Train Loss: 31499193078.8931 | Val Loss: 137127919616.0000\n",
            "Epoch 274/2000000 | Train Loss: 31509239508.8805 | Val Loss: 137188851712.0000\n",
            "Epoch 275/2000000 | Train Loss: 31498028113.7610 | Val Loss: 137055518720.0000\n",
            "Epoch 276/2000000 | Train Loss: 31497105799.9497 | Val Loss: 137032818688.0000\n",
            "Epoch 277/2000000 | Train Loss: 31499997487.2956 | Val Loss: 137072803840.0000\n",
            "Epoch 278/2000000 | Train Loss: 31682936247.2956 | Val Loss: 137038995456.0000\n",
            "Epoch 279/2000000 | Train Loss: 31536580824.5535 | Val Loss: 137231048704.0000\n",
            "Epoch 280/2000000 | Train Loss: 31495592715.8239 | Val Loss: 137073098752.0000\n",
            "Epoch 281/2000000 | Train Loss: 31493563374.9434 | Val Loss: 137042984960.0000\n",
            "Epoch 282/2000000 | Train Loss: 31494211829.1824 | Val Loss: 137048719360.0000\n",
            "Epoch 283/2000000 | Train Loss: 31493291180.9811 | Val Loss: 137080659968.0000\n",
            "Epoch 284/2000000 | Train Loss: 31584683372.0755 | Val Loss: 137018384384.0000\n",
            "Epoch 285/2000000 | Train Loss: 31492919776.5535 | Val Loss: 137041854464.0000\n",
            "Epoch 286/2000000 | Train Loss: 31603361977.4088 | Val Loss: 137050357760.0000\n",
            "Epoch 287/2000000 | Train Loss: 31491265237.5346 | Val Loss: 137027371008.0000\n",
            "Epoch 288/2000000 | Train Loss: 31494543246.5912 | Val Loss: 137045032960.0000\n",
            "Epoch 289/2000000 | Train Loss: 31490564069.3333 | Val Loss: 137058418688.0000\n",
            "Epoch 290/2000000 | Train Loss: 31490324566.1384 | Val Loss: 137038569472.0000\n",
            "Epoch 291/2000000 | Train Loss: 31521349128.1509 | Val Loss: 137084264448.0000\n",
            "Epoch 292/2000000 | Train Loss: 31489452960.1509 | Val Loss: 137054904320.0000\n",
            "Epoch 293/2000000 | Train Loss: 31488758163.4214 | Val Loss: 137375571968.0000\n",
            "Epoch 294/2000000 | Train Loss: 32023238924.9308 | Val Loss: 137000157184.0000\n",
            "Epoch 295/2000000 | Train Loss: 31487043876.6792 | Val Loss: 136998109184.0000\n",
            "Epoch 296/2000000 | Train Loss: 31487819554.3145 | Val Loss: 136998936576.0000\n",
            "Epoch 297/2000000 | Train Loss: 31487395719.4969 | Val Loss: 137032433664.0000\n",
            "Epoch 298/2000000 | Train Loss: 31505099392.6038 | Val Loss: 136984813568.0000\n",
            "Epoch 299/2000000 | Train Loss: 34544979497.0566 | Val Loss: 137009750016.0000\n",
            "Epoch 300/2000000 | Train Loss: 31488748790.4403 | Val Loss: 137185763328.0000\n",
            "Epoch 301/2000000 | Train Loss: 31534497733.6855 | Val Loss: 137016762368.0000\n",
            "Epoch 302/2000000 | Train Loss: 31484447350.5409 | Val Loss: 137315172352.0000\n",
            "Epoch 303/2000000 | Train Loss: 31484303460.7799 | Val Loss: 137029509120.0000\n",
            "Epoch 304/2000000 | Train Loss: 31484118168.5031 | Val Loss: 136978644992.0000\n",
            "Epoch 305/2000000 | Train Loss: 31484058867.1698 | Val Loss: 137033777152.0000\n",
            "Epoch 306/2000000 | Train Loss: 31490841024.0503 | Val Loss: 137068363776.0000\n",
            "Epoch 307/2000000 | Train Loss: 31483117286.1384 | Val Loss: 137023758336.0000\n",
            "Epoch 308/2000000 | Train Loss: 31482802457.9119 | Val Loss: 136988393472.0000\n",
            "Epoch 309/2000000 | Train Loss: 31481933163.0189 | Val Loss: 136999485440.0000\n",
            "Epoch 310/2000000 | Train Loss: 31481925264.8050 | Val Loss: 137015500800.0000\n",
            "Epoch 311/2000000 | Train Loss: 31481927230.1887 | Val Loss: 136999952384.0000\n",
            "Epoch 312/2000000 | Train Loss: 31480955830.0881 | Val Loss: 136992112640.0000\n",
            "Epoch 313/2000000 | Train Loss: 31648992846.6415 | Val Loss: 137001132032.0000\n",
            "Epoch 314/2000000 | Train Loss: 31481435664.8553 | Val Loss: 137012477952.0000\n",
            "Epoch 315/2000000 | Train Loss: 31480271629.3333 | Val Loss: 137003483136.0000\n",
            "Epoch 316/2000000 | Train Loss: 31479109090.8176 | Val Loss: 136985665536.0000\n",
            "Epoch 317/2000000 | Train Loss: 31479436447.6981 | Val Loss: 136992702464.0000\n",
            "Epoch 318/2000000 | Train Loss: 31479998801.8113 | Val Loss: 136997634048.0000\n",
            "Epoch 319/2000000 | Train Loss: 31478753945.4591 | Val Loss: 136995528704.0000\n",
            "Epoch 320/2000000 | Train Loss: 31479152895.8491 | Val Loss: 137025404928.0000\n",
            "Epoch 321/2000000 | Train Loss: 31476580508.6289 | Val Loss: 137013239808.0000\n",
            "Epoch 322/2000000 | Train Loss: 31477819847.2453 | Val Loss: 137000837120.0000\n",
            "Epoch 323/2000000 | Train Loss: 31477296616.8050 | Val Loss: 137011609600.0000\n",
            "Epoch 324/2000000 | Train Loss: 31477573472.4528 | Val Loss: 136994127872.0000\n",
            "Epoch 325/2000000 | Train Loss: 31479168599.6478 | Val Loss: 137007325184.0000\n",
            "Epoch 326/2000000 | Train Loss: 31475451846.8428 | Val Loss: 137053528064.0000\n",
            "Epoch 327/2000000 | Train Loss: 31475664383.3459 | Val Loss: 136970575872.0000\n",
            "Epoch 328/2000000 | Train Loss: 31475691406.9937 | Val Loss: 136972877824.0000\n",
            "Epoch 329/2000000 | Train Loss: 31474562994.8679 | Val Loss: 137104187392.0000\n",
            "Epoch 330/2000000 | Train Loss: 31474621399.4465 | Val Loss: 136964276224.0000\n",
            "Epoch 331/2000000 | Train Loss: 31474751303.4465 | Val Loss: 137077669888.0000\n",
            "Epoch 332/2000000 | Train Loss: 31473816000.9057 | Val Loss: 137027756032.0000\n",
            "Epoch 333/2000000 | Train Loss: 31474121446.0377 | Val Loss: 136956829696.0000\n",
            "Epoch 334/2000000 | Train Loss: 31474827145.6101 | Val Loss: 136983109632.0000\n",
            "Epoch 335/2000000 | Train Loss: 31474866598.6918 | Val Loss: 136957427712.0000\n",
            "Epoch 336/2000000 | Train Loss: 31474431109.4340 | Val Loss: 136990023680.0000\n",
            "Epoch 337/2000000 | Train Loss: 31525807501.0314 | Val Loss: 137049554944.0000\n",
            "Epoch 338/2000000 | Train Loss: 31507523278.7421 | Val Loss: 136971517952.0000\n",
            "Epoch 339/2000000 | Train Loss: 31472507853.5346 | Val Loss: 136973606912.0000\n",
            "Epoch 340/2000000 | Train Loss: 31476446743.8994 | Val Loss: 137002745856.0000\n",
            "Epoch 341/2000000 | Train Loss: 31471163487.7484 | Val Loss: 136959197184.0000\n",
            "Epoch 342/2000000 | Train Loss: 31471309547.1195 | Val Loss: 136997191680.0000\n",
            "Epoch 343/2000000 | Train Loss: 31471589546.4654 | Val Loss: 137014288384.0000\n",
            "Epoch 344/2000000 | Train Loss: 34582642097.9119 | Val Loss: 136964759552.0000\n",
            "Epoch 345/2000000 | Train Loss: 31471079781.5346 | Val Loss: 136982978560.0000\n",
            "Epoch 346/2000000 | Train Loss: 31471557600.9560 | Val Loss: 136956977152.0000\n",
            "Epoch 347/2000000 | Train Loss: 31479423743.0440 | Val Loss: 136954036224.0000\n",
            "Epoch 348/2000000 | Train Loss: 31471381904.6038 | Val Loss: 136988254208.0000\n",
            "Epoch 349/2000000 | Train Loss: 31470059343.1950 | Val Loss: 136966840320.0000\n",
            "Epoch 350/2000000 | Train Loss: 31470242513.7107 | Val Loss: 136984739840.0000\n",
            "Epoch 351/2000000 | Train Loss: 31470289578.0126 | Val Loss: 136984346624.0000\n",
            "Epoch 352/2000000 | Train Loss: 31473955680.9560 | Val Loss: 137063890944.0000\n",
            "Epoch 353/2000000 | Train Loss: 31473704307.2201 | Val Loss: 137051365376.0000\n",
            "Epoch 354/2000000 | Train Loss: 31470453469.8868 | Val Loss: 136953069568.0000\n",
            "Epoch 355/2000000 | Train Loss: 31469621738.7170 | Val Loss: 136969707520.0000\n",
            "Epoch 356/2000000 | Train Loss: 31490478480.5031 | Val Loss: 136969371648.0000\n",
            "Epoch 357/2000000 | Train Loss: 31468567317.2830 | Val Loss: 136962277376.0000\n",
            "Epoch 358/2000000 | Train Loss: 31488448520.0503 | Val Loss: 136972173312.0000\n",
            "Epoch 359/2000000 | Train Loss: 31468454408.0503 | Val Loss: 136972189696.0000\n",
            "Epoch 360/2000000 | Train Loss: 31467529807.1447 | Val Loss: 136984715264.0000\n",
            "Epoch 361/2000000 | Train Loss: 31469195915.7736 | Val Loss: 137093701632.0000\n",
            "Epoch 362/2000000 | Train Loss: 31467929417.9623 | Val Loss: 136979054592.0000\n",
            "Epoch 363/2000000 | Train Loss: 31476840246.6415 | Val Loss: 136955338752.0000\n",
            "Epoch 364/2000000 | Train Loss: 31467203547.3711 | Val Loss: 137012174848.0000\n",
            "Epoch 365/2000000 | Train Loss: 31467103187.0189 | Val Loss: 137001926656.0000\n",
            "Epoch 366/2000000 | Train Loss: 31477053958.3899 | Val Loss: 136938356736.0000\n",
            "Epoch 367/2000000 | Train Loss: 31467390249.8616 | Val Loss: 137102401536.0000\n",
            "Epoch 368/2000000 | Train Loss: 31466093927.3459 | Val Loss: 136992243712.0000\n",
            "Epoch 369/2000000 | Train Loss: 31465174690.9686 | Val Loss: 136947302400.0000\n",
            "Epoch 370/2000000 | Train Loss: 31466908699.0189 | Val Loss: 136982380544.0000\n",
            "Epoch 371/2000000 | Train Loss: 31471770257.7107 | Val Loss: 136948260864.0000\n",
            "Epoch 372/2000000 | Train Loss: 31474855231.4969 | Val Loss: 137058295808.0000\n",
            "Epoch 373/2000000 | Train Loss: 31466718231.3459 | Val Loss: 137045958656.0000\n",
            "Epoch 374/2000000 | Train Loss: 31467538250.4654 | Val Loss: 136974327808.0000\n",
            "Epoch 375/2000000 | Train Loss: 31466316071.5472 | Val Loss: 137103097856.0000\n",
            "Epoch 376/2000000 | Train Loss: 31466743754.9686 | Val Loss: 136937365504.0000\n",
            "Epoch 377/2000000 | Train Loss: 31466068895.3962 | Val Loss: 136998764544.0000\n",
            "Epoch 378/2000000 | Train Loss: 31465564874.1635 | Val Loss: 136937603072.0000\n",
            "Epoch 379/2000000 | Train Loss: 31473930583.0440 | Val Loss: 137040642048.0000\n",
            "Epoch 380/2000000 | Train Loss: 31466016803.5220 | Val Loss: 136974270464.0000\n",
            "Epoch 381/2000000 | Train Loss: 31465235375.6981 | Val Loss: 136984174592.0000\n",
            "Epoch 382/2000000 | Train Loss: 32464039706.6667 | Val Loss: 136967110656.0000\n",
            "Epoch 383/2000000 | Train Loss: 31465099995.5723 | Val Loss: 136957280256.0000\n",
            "Epoch 384/2000000 | Train Loss: 31469482476.7799 | Val Loss: 136977661952.0000\n",
            "Epoch 385/2000000 | Train Loss: 31464999996.3774 | Val Loss: 136985722880.0000\n",
            "Epoch 386/2000000 | Train Loss: 31468769256.6541 | Val Loss: 136972099584.0000\n",
            "Epoch 387/2000000 | Train Loss: 31469898897.6101 | Val Loss: 136935251968.0000\n",
            "Epoch 388/2000000 | Train Loss: 31464852996.9308 | Val Loss: 136966848512.0000\n",
            "Epoch 389/2000000 | Train Loss: 31465623372.8805 | Val Loss: 137022857216.0000\n",
            "Epoch 390/2000000 | Train Loss: 31464016588.4780 | Val Loss: 136934891520.0000\n",
            "Epoch 391/2000000 | Train Loss: 31508926476.6792 | Val Loss: 136966602752.0000\n",
            "Epoch 392/2000000 | Train Loss: 31464409919.0943 | Val Loss: 136983658496.0000\n",
            "Epoch 393/2000000 | Train Loss: 31464229340.5786 | Val Loss: 136990728192.0000\n",
            "Epoch 394/2000000 | Train Loss: 31464460647.6478 | Val Loss: 136952610816.0000\n",
            "Epoch 395/2000000 | Train Loss: 31481671159.8491 | Val Loss: 136949547008.0000\n",
            "Epoch 396/2000000 | Train Loss: 31463491098.9686 | Val Loss: 136963678208.0000\n",
            "Epoch 397/2000000 | Train Loss: 31463687015.5472 | Val Loss: 136972664832.0000\n",
            "Epoch 398/2000000 | Train Loss: 31463715603.4214 | Val Loss: 136946114560.0000\n",
            "Epoch 399/2000000 | Train Loss: 31463575965.5849 | Val Loss: 137061253120.0000\n",
            "Epoch 400/2000000 | Train Loss: 31464253253.7358 | Val Loss: 136967487488.0000\n",
            "Epoch 401/2000000 | Train Loss: 31770290230.1384 | Val Loss: 136952528896.0000\n",
            "Epoch 402/2000000 | Train Loss: 31462647464.3522 | Val Loss: 136951865344.0000\n",
            "Epoch 403/2000000 | Train Loss: 31462890373.0314 | Val Loss: 136973058048.0000\n",
            "Epoch 404/2000000 | Train Loss: 31463319140.0252 | Val Loss: 136949325824.0000\n",
            "Epoch 405/2000000 | Train Loss: 31463375815.7484 | Val Loss: 136969175040.0000\n",
            "Epoch 406/2000000 | Train Loss: 31463182711.8491 | Val Loss: 137041035264.0000\n",
            "Epoch 407/2000000 | Train Loss: 31462298765.0818 | Val Loss: 137000001536.0000\n",
            "Epoch 408/2000000 | Train Loss: 31463832331.9748 | Val Loss: 136937390080.0000\n",
            "Epoch 409/2000000 | Train Loss: 31463702988.1761 | Val Loss: 136922333184.0000\n",
            "Epoch 410/2000000 | Train Loss: 31463797820.4780 | Val Loss: 136956608512.0000\n",
            "Epoch 411/2000000 | Train Loss: 31462963618.6164 | Val Loss: 136932147200.0000\n",
            "Epoch 412/2000000 | Train Loss: 31479799468.0755 | Val Loss: 136962269184.0000\n",
            "Epoch 413/2000000 | Train Loss: 31521741467.2704 | Val Loss: 137005785088.0000\n",
            "Epoch 414/2000000 | Train Loss: 31462610483.9245 | Val Loss: 136944001024.0000\n",
            "Epoch 415/2000000 | Train Loss: 31462261486.3899 | Val Loss: 136991760384.0000\n",
            "Epoch 416/2000000 | Train Loss: 31463050189.8868 | Val Loss: 136959262720.0000\n",
            "Epoch 417/2000000 | Train Loss: 31462398670.7925 | Val Loss: 137040510976.0000\n",
            "Epoch 418/2000000 | Train Loss: 31463059954.0126 | Val Loss: 136956616704.0000\n",
            "Epoch 419/2000000 | Train Loss: 31482671685.9371 | Val Loss: 136951021568.0000\n",
            "Epoch 420/2000000 | Train Loss: 31559058888.9560 | Val Loss: 136948285440.0000\n",
            "Epoch 421/2000000 | Train Loss: 31460636012.5283 | Val Loss: 136982478848.0000\n",
            "Epoch 422/2000000 | Train Loss: 31462646029.4843 | Val Loss: 137180979200.0000\n",
            "Epoch 423/2000000 | Train Loss: 32072671590.9434 | Val Loss: 136919113728.0000\n",
            "Epoch 424/2000000 | Train Loss: 31462216487.6478 | Val Loss: 137099362304.0000\n",
            "Epoch 425/2000000 | Train Loss: 31462194913.4088 | Val Loss: 136951046144.0000\n",
            "Epoch 426/2000000 | Train Loss: 31466104233.4591 | Val Loss: 136957386752.0000\n",
            "Epoch 427/2000000 | Train Loss: 31462474078.4906 | Val Loss: 136941436928.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5156ef69e370>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_axis = range(len(train_losses))\n",
        "plt.plot(x_axis, train_losses, label='Train Loss')\n",
        "plt.plot(x_axis, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "Av-YexkpTJre",
        "outputId": "328d287d-ba60-405e-b7b5-14231c5e6d8c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Should I rescale $y$ as well??"
      ],
      "metadata": {
        "id": "wHvSeWIVTydv"
      }
    }
  ]
}