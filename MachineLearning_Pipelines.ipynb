{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHhgUEaVPQBd0gwF8I2K29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoAldovardi92/Taxi_Project/blob/main/MachineLearning_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2EBqN5yZyBF",
        "outputId": "fbdb97ba-b032-4147-928e-c07ae5c59847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not detected at /content/drive. Attempting to mount...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "mount_point = '/content/drive'\n",
        "\n",
        "# Check if the mount point directory exists and is not empty\n",
        "# A mounted drive will typically have content under its mount point\n",
        "if not os.path.exists(mount_point) or not os.listdir(mount_point):\n",
        "    print(f\"Drive not detected at {mount_point}. Attempting to mount...\")\n",
        "    drive.mount(mount_point)\n",
        "else:\n",
        "    print(f\"Drive already mounted at {mount_point}.\")\n",
        "\n",
        "\n",
        "# Define path inside Google Drive\n",
        "drive_path = '/content/drive/MyDrive/datasets'\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "\n",
        "bog_train_df = pd.read_csv('/content/drive/MyDrive/datasets/bog_train_df.csv')\n",
        "mex_train_df = pd.read_csv('/content/drive/MyDrive/datasets/mex_train_df.csv')\n",
        "uio_train_df = pd.read_csv('/content/drive/MyDrive/datasets/uio_train_df.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1SN0wBZsGZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be kept aside for model evaluation\n",
        "\n",
        "bog_test_df = pd.read_csv('/content/drive/MyDrive/datasets/bog_test_df.csv')\n",
        "mex_test_df = pd.read_csv('/content/drive/MyDrive/datasets/mex_test_df.csv')\n",
        "uio_test_df = pd.read_csv('/content/drive/MyDrive/datasets/uio_test_df.csv')\n",
        "\n",
        "target_column = 'trip_duration'\n",
        "\n",
        "bog_y_test = bog_test_df[target_column]\n",
        "bog_X_test = bog_test_df.drop(columns=[target_column])\n",
        "\n",
        "mex_y_test = mex_test_df[target_column]\n",
        "mex_X_test = mex_test_df.drop(columns=[target_column])\n",
        "\n",
        "uio_y_test = uio_test_df[target_column]\n",
        "uio_X_test = uio_test_df.drop(columns=[target_column])\n",
        "\n",
        "################################################################\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HrdHu8mKb-n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train Validation Splitting\n",
        "# NB: For a Cross validation folding you can always restart from\n",
        "# the datasets id_train and manually split between target and regressors\n",
        "# Given that you have uneven small datasets this could be the best approach\n",
        "####################\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dictionary of your datasets to process\n",
        "# You'll likely get these from your previous step where you performed the initial train/test split\n",
        "# For this example, I'm using the full dataframes assuming you'd have train_df's here\n",
        "dataframes_to_split = {\n",
        "    'bog_train': bog_train_df, # In a real scenario, this would be bog_train_df from the first split\n",
        "    'mex_train': mex_train_df, # mex_train_df\n",
        "    'uio_train': uio_train_df  # uio_train_df\n",
        "}\n",
        "\n",
        "# Dictionary to store the final X_train, X_val, y_train, y_val for each city\n",
        "processed_splits = {}\n",
        "\n",
        "for name, df_train in dataframes_to_split.items():\n",
        "    print(f\"\\n--- Processing {name.upper()} (for Validation Split) ---\")\n",
        "\n",
        "    # Define target and features\n",
        "    target_column = 'trip_duration'\n",
        "    if target_column not in df_train.columns:\n",
        "        print(f\"Error: '{target_column}' not found in {name.upper()} DataFrame. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    y = df_train[target_column]\n",
        "    X = df_train.drop(columns=[target_column])\n",
        "\n",
        "    # Reset indexes for X and y (important if previous operations altered them)\n",
        "    X.reset_index(drop=True, inplace=True)\n",
        "    y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Perform the train-validation split (random split as per your decision)\n",
        "    X_train_final, X_validation, y_train_final, y_validation = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    processed_splits[f'{name}_X_train'] = X_train_final\n",
        "    processed_splits[f'{name}_X_val'] = X_validation\n",
        "    processed_splits[f'{name}_y_train'] = y_train_final\n",
        "    processed_splits[f'{name}_y_val'] = y_validation\n",
        "\n",
        "    print(f\"Split completed for {name.upper()}:\")\n",
        "    print(f\"  X_train shape: {X_train_final.shape}\")\n",
        "    print(f\"  X_val shape: {X_validation.shape}\")\n",
        "    print(f\"  y_train shape: {y_train_final.shape}\")\n",
        "    print(f\"  y_val shape: {y_validation.shape}\")\n",
        "\n",
        "    # Optional: Print some value counts to see distribution, even if not stratified\n",
        "    if 'vendor_id' in X_train_final.columns:\n",
        "        print(f\"\\n  {name.upper()} 'vendor_id' proportions (Train vs. Val):\")\n",
        "        print(\"  Train:\\n\", X_train_final['vendor_id'].value_counts(normalize=True))\n",
        "        print(\"  Validation:\\n\", X_validation['vendor_id'].value_counts(normalize=True))\n",
        "    if 'is_rush_hour' in X_train_final.columns:\n",
        "        print(f\"\\n  {name.upper()} 'is_rush_hour' proportions (Train vs. Val):\")\n",
        "        print(\"  Train:\\n\", X_train_final['is_rush_hour'].value_counts(normalize=True))\n",
        "        print(\"  Validation:\\n\", X_validation['is_rush_hour'].value_counts(normalize=True))\n",
        "\n",
        "\n",
        "print(\"\\n--- All DataFrames processed and split for training/validation ---\")\n",
        "\n",
        "# How to access your split data:\n",
        "# For Bogotá:\n",
        "bog_X_train = processed_splits['bog_train_X_train']\n",
        "bog_X_val = processed_splits['bog_train_X_val']\n",
        "bog_y_train = processed_splits['bog_train_y_train']\n",
        "bog_y_val = processed_splits['bog_train_y_val']\n",
        "\n",
        "# For Mexico City:\n",
        "mex_X_train = processed_splits['mex_train_X_train']\n",
        "mex_X_val = processed_splits['mex_train_X_val']\n",
        "mex_y_train = processed_splits['mex_train_y_train']\n",
        "mex_y_val = processed_splits['mex_train_y_val']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SCgYTBNcVoY",
        "outputId": "fc2100d8-f5a2-4b08-8471-ce4cafc3f2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing BOG_TRAIN (for Validation Split) ---\n",
            "Split completed for BOG_TRAIN:\n",
            "  X_train shape: (1388, 6)\n",
            "  X_val shape: (348, 6)\n",
            "  y_train shape: (1388,)\n",
            "  y_val shape: (348,)\n",
            "\n",
            "  BOG_TRAIN 'vendor_id' proportions (Train vs. Val):\n",
            "  Train:\n",
            " vendor_id\n",
            "Bogotá              0.843660\n",
            "Bogota UberBlack    0.079251\n",
            "Bogotá UberX        0.058357\n",
            "Bogotá UberVan      0.018012\n",
            "Bogotá UberAngel    0.000720\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " vendor_id\n",
            "Bogotá              0.853448\n",
            "Bogota UberBlack    0.080460\n",
            "Bogotá UberX        0.057471\n",
            "Bogotá UberVan      0.005747\n",
            "Bogotá UberAngel    0.002874\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "  BOG_TRAIN 'is_rush_hour' proportions (Train vs. Val):\n",
            "  Train:\n",
            " is_rush_hour\n",
            "False    0.672911\n",
            "True     0.327089\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " is_rush_hour\n",
            "False    0.692529\n",
            "True     0.307471\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Processing MEX_TRAIN (for Validation Split) ---\n",
            "Split completed for MEX_TRAIN:\n",
            "  X_train shape: (6076, 6)\n",
            "  X_val shape: (1519, 6)\n",
            "  y_train shape: (6076,)\n",
            "  y_val shape: (1519,)\n",
            "\n",
            "  MEX_TRAIN 'vendor_id' proportions (Train vs. Val):\n",
            "  Train:\n",
            " vendor_id\n",
            "México DF Taxi Libre       0.559085\n",
            "México DF Taxi de Sitio    0.268433\n",
            "México DF Radio Taxi       0.151415\n",
            "México DF UberX            0.012508\n",
            "México DF UberXL           0.004444\n",
            "México DF UberSUV          0.002140\n",
            "México DF UberBlack        0.001975\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " vendor_id\n",
            "México DF Taxi Libre       0.558920\n",
            "México DF Taxi de Sitio    0.267281\n",
            "México DF Radio Taxi       0.150099\n",
            "México DF UberX            0.014483\n",
            "México DF UberSUV          0.004608\n",
            "México DF UberXL           0.004608\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "  MEX_TRAIN 'is_rush_hour' proportions (Train vs. Val):\n",
            "  Train:\n",
            " is_rush_hour\n",
            "False    0.705234\n",
            "True     0.294766\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " is_rush_hour\n",
            "False    0.705727\n",
            "True     0.294273\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Processing UIO_TRAIN (for Validation Split) ---\n",
            "Split completed for UIO_TRAIN:\n",
            "  X_train shape: (17068, 6)\n",
            "  X_val shape: (4268, 6)\n",
            "  y_train shape: (17068,)\n",
            "  y_val shape: (4268,)\n",
            "\n",
            "  UIO_TRAIN 'vendor_id' proportions (Train vs. Val):\n",
            "  Train:\n",
            " vendor_id\n",
            "Quito                     0.986232\n",
            "Quito Cabify Lite         0.011718\n",
            "Quito Cabify Executive    0.001992\n",
            "Quito UberX               0.000059\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " vendor_id\n",
            "Quito                     0.988519\n",
            "Quito Cabify Lite         0.010309\n",
            "Quito Cabify Executive    0.001172\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "  UIO_TRAIN 'is_rush_hour' proportions (Train vs. Val):\n",
            "  Train:\n",
            " is_rush_hour\n",
            "False    0.644129\n",
            "True     0.355871\n",
            "Name: proportion, dtype: float64\n",
            "  Validation:\n",
            " is_rush_hour\n",
            "False    0.632615\n",
            "True     0.367385\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- All DataFrames processed and split for training/validation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "\n",
        "# Define your columns\n",
        "categorical_columns = ['vendor_id', 'is_rush_hour']\n",
        "# Ensure numerical_columns are correctly identified based on your DataFrame's columns\n",
        "numerical_columns = [col for col in bog_X_train.columns if col not in categorical_columns]\n",
        "\n",
        "\n",
        "# Create the preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "        ('num', StandardScaler(), numerical_columns)\n",
        "    ])\n",
        "\n",
        "# Define the full pipeline\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', Ridge())\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gs8FRL-7jK_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install itable\n",
        "#import pandas as pd\n",
        "#from itables import init_notebook_mode, show\n",
        "\n",
        "# Enable itables to display DataFrames interactively\n",
        "# This only needs to be run once per session\n",
        "#init_notebook_mode(all_interactive=True)\n",
        "\n",
        "# Now, simply display your bog_X_train DataFrame.\n",
        "# It will be interactive and scrollable after init_notebook_mode is enabled.\n",
        "#bog_X_train\n",
        "\n",
        "# Alternatively, if you only want specific DataFrames to be interactive:\n",
        "# show(bog_X_train)"
      ],
      "metadata": {
        "id": "9Y3bH2inK1wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325b639f"
      },
      "source": [
        "# Fit the pipeline on the training data\n",
        "model_pipeline.fit(bog_X_train, bog_y_train)\n",
        "model_pipeline.fit(mex_X_train, mex_y_train)\n",
        "model_pipeline.fit(uio_X_train, uio_y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}